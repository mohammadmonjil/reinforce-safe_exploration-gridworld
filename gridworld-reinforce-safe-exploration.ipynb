{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "The main requirements are PyTorch (of course), and numpy, matplotlib, and iPython for animating the states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import count\n",
    "import random\n",
    "import time \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from torch.autograd import Variable\n",
    "from collections import namedtuple, deque\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque\n",
    "\n",
    "\n",
    "import matplotlib.mlab as mlab\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "from IPython.display import HTML\n",
    "import math\n",
    "\n",
    "%matplotlib\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from helpers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state','flag'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOAL_VALUE = 175\n",
    "WATER_VALUE = 50\n",
    "WALL_VALUE = 125\n",
    "AGENT_HEALTH = 200\n",
    "VISIBLE_RADIUS = 1\n",
    "history_length = 500000\n",
    "\n",
    "GAME_ART = [\n",
    "     'WWWWW###',\n",
    "     'W      W',\n",
    "     'W  A   W',\n",
    "     'W      W',\n",
    "     'W      W',\n",
    "     'W   G  W',\n",
    "     'W      W',\n",
    "     '########']\n",
    "\n",
    "yaxis = 8\n",
    "xaxis = 8\n",
    "\n",
    "grid = np.zeros((xaxis,yaxis))\n",
    "\n",
    "for i in range(0, xaxis):\n",
    "    for j in range(0,yaxis):\n",
    "            \n",
    "        if GAME_ART[i][j] == 'W':\n",
    "            grid[i][j] = WATER_VALUE\n",
    "        if GAME_ART[i][j] == '#':\n",
    "            grid[i][j] = WALL_VALUE\n",
    "        if GAME_ART[i][j] == 'A':\n",
    "            agentx, agenty = (i,j)\n",
    "        if GAME_ART[i][j] == 'G':\n",
    "            grid[i][j]= GOAL_VALUE\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Grid():\n",
    "    def __init__(self):\n",
    "        self.grid = grid\n",
    "        self.grid_size = 8\n",
    "        #self.grid_size = math.ceil((yaxis**2 + xaxis**2)**0.5)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.grid = grid\n",
    "    \n",
    "    def visible(self, pos):\n",
    "        y, x = pos\n",
    "        return self.grid[y-VISIBLE_RADIUS:y+VISIBLE_RADIUS+1, x-VISIBLE_RADIUS:x+VISIBLE_RADIUS+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3 \n",
    "\n",
    "class Agent:\n",
    "    def reset(self):\n",
    "        self.health = AGENT_HEALTH\n",
    "\n",
    "    def act(self, action):\n",
    "        # Move according to action: 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
    "        y, x = self.pos\n",
    "        if action == UP: y -= 1\n",
    "        elif action == RIGHT: x += 1\n",
    "        elif action == DOWN: y += 1\n",
    "        elif action == LEFT: x -= 1\n",
    "        self.pos = (y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_REWARD = 0\n",
    "WALL_HIT_REWARD = -2\n",
    "WATER_REWARD = -50\n",
    "GOAL_REWARD = 50\n",
    "\n",
    "class Environment(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.grid = Grid()\n",
    "        self.agent = Agent()\n",
    "        self.safety_score = 5\n",
    "        self.wall_hit = False\n",
    "        self.success = False\n",
    "        \n",
    "        \n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Start a new episode by resetting grid and agent\"\"\"\n",
    "        self.grid.reset()\n",
    "        self.agent.reset()\n",
    "        self.wall_hit = False\n",
    "        self.success = False\n",
    "        \n",
    "        #c = math.floor(self.grid.grid_size / 2)\n",
    "        self.agent.pos = (agentx, agenty)\n",
    "        y, x = self.agent.pos\n",
    "        self.safety_score = calculate_min_distance(self.grid.grid, y, x)\n",
    "        \n",
    "        self.t = 0\n",
    "        self.history = deque( maxlen = history_length )\n",
    "        self.record_step()\n",
    "        \n",
    "        return self.visible_state\n",
    "    \n",
    "    def record_step(self):\n",
    "        \"\"\"Add the current state to history for display later\"\"\"\n",
    "        grid = np.array(self.grid.grid)\n",
    "        grid[self.agent.pos] = self.agent.health  # Agent marker faded by health\n",
    "        visible = np.array(self.grid.visible(self.agent.pos),dtype=\"float32\")\n",
    "        #print(visible)\n",
    "        self.history.append((grid, visible, self.agent.health, self.safety_score))\n",
    "    \n",
    "    @property\n",
    "    def visible_state(self):\n",
    "        \"\"\"Return the visible area surrounding the agent, and current agent health\"\"\"\n",
    "        visible = self.grid.visible(self.agent.pos)\n",
    "        y, x = self.agent.pos\n",
    "        yp = (y - VISIBLE_RADIUS) / self.grid.grid_size\n",
    "        xp = (x - VISIBLE_RADIUS) / self.grid.grid_size\n",
    "        \n",
    "        self.safety_score = calculate_min_distance(self.grid.grid, y, x)\n",
    "        \n",
    "        extras = [yp, xp]\n",
    "        \n",
    "        \n",
    "        return np.concatenate((visible.flatten(), extras), 0)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Update state (grid and agent) based on an action\"\"\"\n",
    "        won = lost = False\n",
    "        tempy, tempx = self.agent.pos # save current agent position temporarility\n",
    "        \n",
    "        self.agent.act(action)\n",
    "        \n",
    "        # Get reward from where agent landed\n",
    "        value = self.grid.grid[self.agent.pos]\n",
    "        \n",
    "        # Check if agent won (reached the goal) or lost (dropped into water)\n",
    "        \n",
    "        if value == GOAL_VALUE:\n",
    "            won = True\n",
    "        elif value == WATER_VALUE:\n",
    "            lost = True\n",
    "        \n",
    "        done = won or lost\n",
    "        \n",
    "        # Check if agent has hit the wall\n",
    "        if value == WALL_VALUE:\n",
    "            self.wall_hit =True\n",
    "        \n",
    "        \n",
    "        if done and won:\n",
    "            reward = GOAL_REWARD\n",
    "            self.success = True\n",
    "        elif done and lost:\n",
    "            self.safety_score = reward = WATER_REWARD\n",
    "        elif self.wall_hit:\n",
    "            reward = WALL_HIT_REWARD\n",
    "            self.agent.pos = (tempy, tempx) # agent stays where it was because of wall\n",
    "            y, x = self.agent.pos\n",
    "            self.wall_hit = False\n",
    "        else:\n",
    "            reward = STEP_REWARD\n",
    "            y, x = self.agent.pos\n",
    "            self.safety_score = calculate_min_distance(self.grid.grid, y, x)\n",
    "\n",
    "        # Save in history\n",
    "        self.record_step()\n",
    "        \n",
    "        return self.visible_state, reward, done, won, lost\n",
    "\n",
    "\n",
    "def calculate_min_distance(grid, agent_posy, agent_posx):\n",
    "\n",
    "    safety_score = 1000\n",
    "    for index_y, item in enumerate(grid):   # default is zero\n",
    "        for index_x, val in enumerate(item):\n",
    "        \n",
    "            if val == WATER_VALUE:\n",
    "            \n",
    "                current_distance = ((index_y-agent_posy)**2+(index_x-agent_posx)**2)**0.5 \n",
    "            \n",
    "                if current_distance < safety_score:\n",
    "                    safety_score = current_distance\n",
    "    \n",
    "    return(safety_score)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate(history):\n",
    "    frames = len(history)\n",
    "    print(\"Rendering %d frames...\" % frames)\n",
    "    fig = plt.figure(figsize=(6, 2))\n",
    "    fig_grid = fig.add_subplot(121)\n",
    "\n",
    "\n",
    "    def render_frame(i):\n",
    "        grid, visible, health, safety_score = history[i]\n",
    "        \n",
    "        # Render grid\n",
    "        fig_grid.matshow(grid)\n",
    "\n",
    "    anim = matplotlib.animation.FuncAnimation(\n",
    "        fig, render_frame, frames=frames, interval=100\n",
    "    )\n",
    "\n",
    "    plt.close()\n",
    "    display(HTML(anim.to_html5_video()))\n",
    "    anim.save(\"TLI.gif\", writer='imagemagick',fps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Environment\n",
    "\n",
    "Let's test what we have so far with a quick simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = Environment()\n",
    "# env.reset()\n",
    "# #print(env.visible_state)\n",
    "\n",
    "# done = False\n",
    "# while not done:\n",
    "#     _, _, done, safety_score = env.step(RIGHT) # taking action Down\n",
    "#     print(env.safety_score)\n",
    "\n",
    "# animate(env.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visible_squares = (VISIBLE_RADIUS * 2 + 1) ** 2\n",
    "input_size = visible_squares + 2 # Plus agent position y, x\n",
    "hidden_size = 128\n",
    "action_size = 4\n",
    "\n",
    "hidden_size_fear_net = 128\n",
    "    \n",
    "class Policy(nn.Module):\n",
    "    \"\"\"\n",
    "    implements both actor and critic in one model\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(input_size, hidden_size)\n",
    "\n",
    "        # actor's layer\n",
    "        self.action_head = nn.Linear(hidden_size, action_size)\n",
    "\n",
    "        # critic's layer\n",
    "        self.value_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        # action & reward buffer\n",
    "        self.saved_states = []\n",
    "        self.saved_actions = []\n",
    "        self.output_probs = []\n",
    "        self.rewards = []\n",
    "        self.lost_mask = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward of both actor and critic\n",
    "        \"\"\"\n",
    "        x = F.relu((self.affine1(x)))\n",
    "\n",
    "        # actor: choses action to take from state s_t\n",
    "        # by returning probability of each action\n",
    "        action_prob = F.softmax(self.action_head(x), dim=-1)\n",
    "\n",
    "        # critic: evaluates being in the state s_t\n",
    "        state_values = self.value_head(x)\n",
    "\n",
    "        # return values for both actor and critic as a tuple of 2 values:\n",
    "        # 1. a list with the probability of each action over the action space\n",
    "        # 2. the value from state s_t\n",
    "        return action_prob, state_values\n",
    "    \n",
    "class Fear_Network(nn.Module):\n",
    "    \"\"\"\n",
    "    implements the fear network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Fear_Network, self).__init__()\n",
    "        self.affine = nn.Linear(input_size, hidden_size_fear_net)\n",
    "\n",
    "        # actor's layer\n",
    "        self.output = nn.Linear(hidden_size_fear_net, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu((self.affine(x)))\n",
    "\n",
    "        danger_flag = self.output(x)\n",
    "\n",
    "        return danger_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    \n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs, state_value = model(state)\n",
    "    \n",
    "    \n",
    "\n",
    "    # create a categorical distribution over the list of probabilities of actions\n",
    "    m = Categorical(probs)\n",
    "\n",
    "    # and sample an action using the distribution\n",
    "    action = m.sample()\n",
    "    \n",
    "    model.saved_states.append(state)\n",
    "\n",
    "    # save to action buffer\n",
    "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "    \n",
    "    #save the output probability of policy head\n",
    "    model.output_probs.append(probs.detach().numpy())\n",
    "\n",
    "    # the action to take (left, right, up, down)\n",
    "    \n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        return action.item()\n",
    "    else:\n",
    "        return random.randrange(action_size)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    \"\"\"\n",
    "    Training code. Calculates actor and critic loss and performs backprop.\n",
    "    \"\"\"\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    saved_output_probs = model.output_probs\n",
    "    saved_states = model.saved_states\n",
    "    lost_mask = model.lost_mask\n",
    "    #print(saved_output_probs)\n",
    "    policy_losses = [] # list to save actor (policy) loss\n",
    "    value_losses = [] # list to save critic (value) loss\n",
    "    entropy_losses = []\n",
    "    returns = [] # list to save the true values\n",
    "\n",
    "    # calculate the true value using rewards returned from the environment\n",
    "    for r in model.rewards[::-1]:\n",
    "        # calculate the discounted value\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "    \n",
    "    for (log_prob, value), R, output_probs, saved_state, lost in zip(saved_actions, returns,saved_output_probs, saved_states, lost_mask):\n",
    "        \n",
    "        if lost:\n",
    "            fear = torch.tensor([fear_factor])\n",
    "            \n",
    "        else:\n",
    "            fear = fear_factor*fear_net(saved_state)\n",
    "        \n",
    "        \n",
    "        advantage = R - value.item() - fear\n",
    "        # calculate actor (policy) loss\n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "        \n",
    "        #calculate the entropy loss\n",
    "        entropy_losses.append(torch.tensor(-beta*(np.log(output_probs)*output_probs).sum()))\n",
    "\n",
    "        # calculate critic (value) loss using L1 smooth loss\n",
    "        value_losses.append(zeta*F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    total_loss = torch.stack(policy_losses).sum().mean() + torch.stack(value_losses).sum().mean() - torch.stack(entropy_losses).sum().mean()\n",
    "\n",
    "    \n",
    "    total_loss.backward()\n",
    "    \n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "    # reset rewards and action buffer\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]\n",
    "    del model.output_probs[:]\n",
    "    \n",
    "\n",
    "def optimize_fear():\n",
    "    if len(unsafe_buffer) < math.floor(BATCH_SIZE/2) or len(safe_buffer) < math.floor(BATCH_SIZE/2):\n",
    "        return\n",
    "    #print(len(unsafe_buffer))\n",
    "    unsafe = random.sample(unsafe_buffer, math.floor(BATCH_SIZE/2))\n",
    "    safe = random.sample(safe_buffer, math.floor(BATCH_SIZE/2))\n",
    "    state_batch = unsafe + safe\n",
    "    state_batch = torch.Tensor(np.array(state_batch))\n",
    "\n",
    "    \n",
    "    flag_batch = []\n",
    "    for i in range(0,math.floor(BATCH_SIZE/2)):\n",
    "        flag_batch = flag_batch+[[1]]\n",
    "    for i in range(0,math.floor(BATCH_SIZE/2)):\n",
    "        flag_batch = flag_batch+[[0]]\n",
    "        \n",
    "    flag_batch = torch.Tensor(flag_batch)\n",
    "        \n",
    "    state_batch = unsafe + safe\n",
    "    \n",
    "    state_batch = torch.Tensor(np.array(state_batch))\n",
    "    \n",
    "\n",
    "    policy_output_batch = fear_net(state_batch)\n",
    "    loss_calculator = nn.MSELoss()\n",
    "    loss = loss_calculator(flag_batch,policy_output_batch)\n",
    "    optimizer_fear.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_fear.step()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "model = Policy()\n",
    "fear_net = Fear_Network()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "optimizer_fear = optim.Adam(fear_net.parameters(), lr=1e-3)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "safe_buffer = deque([],maxlen=10000)\n",
    "unsafe_buffer = deque([],maxlen=10000)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "history = deque( maxlen=history_length )\n",
    "running_reward = 0\n",
    "num_episodes = 100000\n",
    "beta = 0\n",
    "zeta = .01\n",
    "\n",
    "seed = 543\n",
    "gamma = 0.99 \n",
    "torch.manual_seed(seed)\n",
    "\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.001\n",
    "EPS_DECAY = math.floor(num_episodes*.75)\n",
    "steps_done = 0\n",
    "\n",
    "fear_radius = 2\n",
    "fear_factor = 1\n",
    "\n",
    "log_interval = 100\n",
    "\n",
    "total_success = 0\n",
    "total_catastrophe = 0\n",
    "episode_reward = []\n",
    "average_reward = []\n",
    "catastrophe_count = []\n",
    "\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and state\n",
    "    ep_reward = 0\n",
    "    env.reset()\n",
    "    state = env.visible_state\n",
    "    \n",
    "    state_list = []\n",
    "    \n",
    "    for t in range(1, 150):\n",
    "\n",
    "        # select action from policy\n",
    "        action = select_action(state)\n",
    "\n",
    "        # take the action\n",
    "        next_state, reward, done, won, lost = env.step(action)\n",
    "\n",
    "        state_list.append(state)\n",
    "        model.rewards.append(reward)\n",
    "        ep_reward += reward\n",
    "        \n",
    "        if lost:\n",
    "            model.lost_mask.append(1)\n",
    "        else:\n",
    "            model.lost_mask.append(0)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        state = next_state \n",
    "    \n",
    "    if lost:\n",
    "        total_catastrophe +=1\n",
    "        \n",
    "        \n",
    "        if fear_radius < t:\n",
    "            num_safe_states = t - fear_radius\n",
    "        else:\n",
    "            num_safe_states = 0\n",
    "            \n",
    "        for i in range(0,num_safe_states):\n",
    "            safe_buffer.append(state_list[i])\n",
    "        \n",
    "        for i in range(num_safe_states,t):\n",
    "            unsafe_buffer.append(state_list[i])\n",
    "    else:\n",
    "        model.lost_mask.append(0)\n",
    "    \n",
    "    running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "    catastrophe_count.append(total_catastrophe)\n",
    "    episode_reward.append(ep_reward)\n",
    "    \n",
    "    if won:\n",
    "        print(\"goal reached in episode\",i_episode,\"reward\", ep_reward,\"running reward\", running_reward)\n",
    "        total_success += 1\n",
    "\n",
    "        # perform backprop\n",
    "    optimize_fear()\n",
    "    finish_episode()\n",
    "\n",
    "#     print(\"episode no \",i_episode, \"success\", total_success, \"failed \",total_catastrophe)\n",
    "    \n",
    "#    history.append(env.history)\n",
    "    \n",
    "    if i_episode % log_interval == 0:\n",
    "        print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f} success {:.2f} failure {:.2f}'\n",
    "              .format(i_episode, ep_reward, running_reward,total_success,total_catastrophe))\n",
    "\n",
    "\n",
    "print('Complete')\n",
    "animate(env.history)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
